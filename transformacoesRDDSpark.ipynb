{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformacoesRDDSpark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBvGckrz7QGtJIRAFZ4uwi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nararodriguess/transformacoes_acoes_rdd/blob/main/transformacoesRDDSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principais transformações que podem ser executadas sobre RDDs do Spark ✅\n",
        "\n",
        "\n",
        "*     A operação de Transformação do Spark produz um ou mais novos RDDs.\n"
      ],
      "metadata": {
        "id": "IMcd5Q9KzDBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8xZI5S40u5J",
        "outputId": "65edf658-e540-43da-b650-f8c65732195e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 56.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=311d38d4137a6ceadadf9654f9c2e274a90308e5df45cdc888c43be584a59d6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XOOFJHSJyreV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "from operator import add"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**map()**: Cria um novo RDD passando cada elemento do RDD de origem pela função func. Cada elemento do RDD de origem é mapeado em um único item no novo RDD:"
      ],
      "metadata": {
        "id": "EuOyeE1rzSbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession.builder.appName(\"Map\").getOrCreate()) #Cria sessão"
      ],
      "metadata": {
        "id": "SIk7xPyY4ViZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fileWords = \"palavras2.txt\" # atribui à variável fileWords o arquivo .txt contendo um texto\n",
        "data = spark.read.text(fileWords).rdd.map(lambda r: r[0]) # transforma.txt em RDD usando o map()"
      ],
      "metadata": {
        "id": "gQseLu7r4sVb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**flatMap()**: Similar ao map(), mapeia os elementos e retorna um novo RDD aplicando primeiro uma função a todos os elementos desse RDD e, em seguida, nivelando os resultados:"
      ],
      "metadata": {
        "id": "JgxIxDMV8Ymu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileWords = data.flatMap(lambda line : line.split()) #Atribui à variável fileFlat o arquivo data, mapeia o arquivo por meio do flatMap()\n",
        "output = fileWords.collect()\n",
        "for i in output:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRHrxAfR5LWb",
        "outputId": "1d997208-8ca2-4223-88a9-d9299d3f02d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unify\n",
            "the\n",
            "processing\n",
            "of\n",
            "your\n",
            "data\n",
            "in\n",
            "batches\n",
            "and\n",
            "real-time\n",
            "streaming,\n",
            "using\n",
            "your\n",
            "preferred\n",
            "language:\n",
            "Python,\n",
            "SQL,\n",
            "Scala,\n",
            "Java\n",
            "or\n",
            "R.\n",
            "Execute\n",
            "fast,\n",
            "distributed\n",
            "ANSI\n",
            "SQL\n",
            "queries\n",
            "for\n",
            "dashboarding\n",
            "and\n",
            "ad-hoc\n",
            "reporting.\n",
            "Runs\n",
            "faster\n",
            "than\n",
            "most\n",
            "data\n",
            "warehouses.\n",
            "Perform\n",
            "Exploratory\n",
            "Data\n",
            "Analysis\n",
            "(EDA)\n",
            "on\n",
            "petabyte-scale\n",
            "data\n",
            "without\n",
            "having\n",
            "to\n",
            "resort\n",
            "to\n",
            "downsampling.\n",
            "Train\n",
            "machine\n",
            "learning\n",
            "algorithms\n",
            "on\n",
            "a\n",
            "laptop\n",
            "and\n",
            "use\n",
            "the\n",
            "same\n",
            "code\n",
            "to\n",
            "scale\n",
            "to\n",
            "fault-tolerant\n",
            "clusters\n",
            "of\n",
            "thousands\n",
            "of\n",
            "machines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter()**: Cria um novo RDD contendo apenas itens que foram retornados como verdadeiro, conforme a função:"
      ],
      "metadata": {
        "id": "2T6ERk9MCrvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileWords = data.flatMap(lambda line : line.split()).filter(lambda word: word.startswith(\"a\")) # filter() filtra o RDD para palavras que começam com 'a'\n",
        "output = fileWords.collect()\n",
        "for i in output:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZExw1NtOC2f1",
        "outputId": "d8693cd2-01a3-4d94-cee9-f84da571fcc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and\n",
            "and\n",
            "ad-hoc\n",
            "algorithms\n",
            "a\n",
            "and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduceByKey()**: Redução pela chave, agrega todas as tuplas:"
      ],
      "metadata": {
        "id": "0nsObMCSDbt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"maria\", \"jose\", \"jose\", \"joao\", \"joao\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = (rdd.map(lambda w: (w,1)).reduceByKey(add)).collect()\n",
        "for i in rdd2:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf5-nxwDDmgJ",
        "outputId": "06c735c8-0e38-4ff7-88d0-9ce5bad99d8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('joao', 2)\n",
            "('maria', 1)\n",
            "('jose', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sortByKey()**: ordena as tuplas de acordo com a chave:"
      ],
      "metadata": {
        "id": "fsYznc03Eg61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"um\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = (rdd.map(lambda w: (w,1)).reduceByKey(add).sortByKey(\"asc\")).collect()\n",
        "for i in rdd2:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao9ZJ5gDEUDe",
        "outputId": "961b058d-7e42-4aeb-9388-a8ba799aab6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('cinco', 1)\n",
            "('dois', 2)\n",
            "('um', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**union(rdd)**: Cria um RDD que contem todos os elementos dos RDDs. Os objetos dos RDDs devem ser do mesmo tipo:"
      ],
      "metadata": {
        "id": "dz6-qZDxE3fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"um\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "list2 = [\"tres\", \"tres\", \"quantro\", \"zero\", \"zero\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = spark.sparkContext.parallelize(list2)\n",
        "rddUnion = rdd.union(rdd2).collect()\n",
        "for i in rddUnion:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew8m5OVsFNAG",
        "outputId": "f3d9c18a-5cc6-4b80-bfdf-03d26f8208f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "um\n",
            "dois\n",
            "cinco\n",
            "um\n",
            "dois\n",
            "tres\n",
            "tres\n",
            "quantro\n",
            "zero\n",
            "zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**intersection(rdd)**: Cria um RDD com elementos comuns entre os RDDs:"
      ],
      "metadata": {
        "id": "oiXjS_dwF2eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"um\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "list2 = [\"um\", \"cinco\", \"quantro\", \"cinco\", \"zero\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = spark.sparkContext.parallelize(list2)\n",
        "rddInter = rdd.intersection(rdd2).collect()\n",
        "for i in rddInter:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUYC_YSTGBA8",
        "outputId": "3e5f08bb-b701-4b7a-9a8d-acb362c6e354"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "um\n",
            "cinco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**distinct(rdd)**: Cria um RDD com itens distintos:"
      ],
      "metadata": {
        "id": "_-Y3fkgLGQCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"um\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list).distinct().collect()\n",
        "for i in rdd:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5uhSAMvGe3d",
        "outputId": "9ee46aca-13c8-4e5e-ceb2-a75a89301205"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "um\n",
            "dois\n",
            "cinco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**join)rdd)**: Junção de tuplas com mesma chave:"
      ],
      "metadata": {
        "id": "D1U5ZE31Gq43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nome = [(\"Joao\", 25), (\"Lucas\", 18)]\n",
        "cidade = [(\"Joao\", \"SP\"), (\"Lucas\", \"RJ\")]\n",
        "\n",
        "rddNome = spark.sparkContext.parallelize(nome)\n",
        "rddCidade = spark.sparkContext.parallelize(cidade)\n",
        "\n",
        "rddJoin = rddNome.join(rddCidade).collect()\n",
        "print(rddJoin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBmAXITjGpEe",
        "outputId": "9ca0fd64-10e5-4c7e-8d9e-2cdd23ec3dc3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Joao', (25, 'SP')), ('Lucas', (18, 'RJ'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principais ações que podem ser executadas sobre RDDs do Spark ✅\n",
        "\n",
        "\n",
        "*   Ações são operações que produzem valores que não são RDDs\n",
        "*   Os valores produzidos são copiados para o *driver* ou para o sistema de armazenamento"
      ],
      "metadata": {
        "id": "4Eg5mVlFH2Ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()**: Retorna o conteúdo do RDD para o driver:"
      ],
      "metadata": {
        "id": "k05aOF0DI9RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddJoin = rddNome.join(rddCidade).collect()\n",
        "print(rddJoin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgdugf9rILxe",
        "outputId": "1f273cb6-4e12-4f24-ecef-e59120e1545b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Joao', (25, 'SP')), ('Lucas', (18, 'RJ'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count()**: Conta o número de itens do RDD:"
      ],
      "metadata": {
        "id": "uYJ4S49nJPD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nome = [(\"Joao\", 25), (\"Lucas\", 18)]\n",
        "cidade = [(\"Joao\", \"SP\"), (\"Lucas\", \"RJ\")]\n",
        "\n",
        "rddNome = spark.sparkContext.parallelize(nome)\n",
        "rddCidade = spark.sparkContext.parallelize(cidade)\n",
        "\n",
        "rddJoin = rddNome.join(rddCidade)\n",
        "print(rddJoin.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZs_3jzyJMR3",
        "outputId": "3f29be8c-2e9b-4f6b-f8bc-56ccab95f6cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**take(in)**: Retorna N elementos aleatórios do RDD:"
      ],
      "metadata": {
        "id": "eqVK4ldNJn1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"dois\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "list2 = [\"um\", \"cinco\", \"quantro\", \"cinco\", \"zero\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = spark.sparkContext.parallelize(list2)\n",
        "rddUni = rdd.union(rdd2)\n",
        "rddUni.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AVu3E6gJ5oZ",
        "outputId": "ba3e37f8-75fb-4187-928b-0f775f49c290"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dois', 'dois', 'cinco']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**top(k)**: Retorna k elementos considerando a ordem decrescente:"
      ],
      "metadata": {
        "id": "AAenZe8VKWmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"dois\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "list2 = [\"um\", \"cinco\", \"quantro\", \"cinco\", \"zero\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = spark.sparkContext.parallelize(list2)\n",
        "rddUni = rdd.union(rdd2)\n",
        "rddUni.top(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRPBwRpMKcI_",
        "outputId": "8432aa76-2268-43b2-ec94-df6397f8917f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['zero', 'um', 'um', 'quantro', 'dois', 'dois', 'dois']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**countByValue()**: Conta o número de ocorrências para cada valor do RDD:"
      ],
      "metadata": {
        "id": "RXQc6jKJLPI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"dois\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "list2 = [\"um\", \"cinco\", \"quantro\", \"cinco\", \"zero\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = spark.sparkContext.parallelize(list2)\n",
        "rddUni = rdd.union(rdd2)\n",
        "rddUni.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obTZylwFL5jI",
        "outputId": "939c3551-b414-41a0-8895-79a26b56d6d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'cinco': 3, 'dois': 3, 'quantro': 1, 'um': 2, 'zero': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduce()**: Executa uma operação para todos os itens do RDD:"
      ],
      "metadata": {
        "id": "53Cqn1yfOdDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"dois\", \"dois\", \"cinco\", \"um\", \"dois\"]\n",
        "list2 = [\"um\", \"cinco\", \"quantro\", \"cinco\", \"zero\"]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(list)\n",
        "rdd2 = spark.sparkContext.parallelize(list2)\n",
        "rddUni = rdd.union(rdd2)\n",
        "rddUni.reduce(lambda a,b: a + ' ' + b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k-NSbxNCMPxz",
        "outputId": "ef4b27d1-c365-4d36-968c-9f02ef205459"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dois dois cinco um dois um cinco quantro cinco zero'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}